{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColorHandPose3D Demo\n",
    "\n",
    "This notebook demos the ColorHandPose3d network as implemented in \"Learning to Estimate 3D Hand Pose from Single RGB Images\" by Zimmerman et al. Their project is available at [https://github.com/lmb-freiburg/hand3d]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "\n",
    "model_path = os.path.abspath(os.path.join('..'))\n",
    "if model_path not in sys.path:\n",
    "    sys.path.append(model_path)\n",
    "    \n",
    "from colorhandpose3d.model.ColorHandPose3D import ColorHandPose3D\n",
    "from colorhandpose3d.utils.general import *\n",
    "from colorhandpose3d.utils.transforms import *\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize models and load weights\n",
    "\n",
    "ColorHandPose3d consists of 4 networks:\n",
    "- HandSegNet\n",
    "- PoseNet\n",
    "- PosePrior\n",
    "- ViewPointNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = '../saved/'\n",
    "\n",
    "chp3d = ColorHandPose3D(weight_path, 224)\n",
    "if use_cuda is True:\n",
    "    chp3d.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and run sample\n",
    "\n",
    "The network is trained on the RHD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform0 = torchvision.transforms.ToPILImage()\n",
    "transform1 = torchvision.transforms.ToTensor()\n",
    "transform2 = torchvision.transforms.Resize((224, 224))\n",
    "# img = Image.open('/data/RHD_v1-1/RHD_published_v2/training/color/07689.png') # bad image\n",
    "    \n",
    "img = Image.open('../outputs/in_video/0172.png')\n",
    "img = Image.open('../outputs/in_video/0883.png')\n",
    "img = Image.open('../outputs/in_video/0194.png')\n",
    "img = Image.open('../outputs/in_video/0290.png')\n",
    "img = Image.open('../outputs/in_video/0435.png')\n",
    "img = Image.open('../outputs/in_video/0764.png')\n",
    "img = Image.open('../outputs/in_video/0788.png')\n",
    "print(img)\n",
    "sample_original = transform1(transform2(img)).unsqueeze(0)\n",
    "print(sample_original.shape)\n",
    "sample = sample_original - 0.5\n",
    "hand_side = torch.tensor([[1.0, 0.0]])\n",
    "\n",
    "# Cuda\n",
    "if use_cuda is True:\n",
    "    sample = sample.cuda()\n",
    "    hand_side = hand_side.cuda()\n",
    "\n",
    "# Run through network\n",
    "import time\n",
    "inps = [None, sample]\n",
    "s = time.time()\n",
    "coords_xyz_rel_normed, keypoint_scoremap, image_crop, centers, scale_crop, hand_mask = chp3d(inps, hand_side)\n",
    "e = time.time()\n",
    "print('Total forward time: {}'.format(e-s))\n",
    "\n",
    "# Back to CPU\n",
    "if use_cuda is True:\n",
    "    coords_xyz_rel_normed = coords_xyz_rel_normed.cpu()\n",
    "    keypoint_scoremap = keypoint_scoremap.cpu()\n",
    "    image_crop = image_crop.cpu()\n",
    "    centers = centers.cpu()\n",
    "    scale_crop = scale_crop.cpu()\n",
    "    hand_mask = hand_mask.cpu()\n",
    "\n",
    "keypoint_coords3d = coords_xyz_rel_normed.detach().numpy()\n",
    "keypoint_coords3d = keypoint_coords3d.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_mask_img = transform0(hand_mask.squeeze(0))\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot()\n",
    "ax1.imshow(hand_mask_img)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2img(fig):\n",
    "    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n",
    "    import io\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, bbox_inches='tight', pad_inches=0)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the output\n",
    "\n",
    "Display the crop, heatmaps, and estimated pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get other things to visualize\n",
    "keypoint_coords_crop = detect_keypoints(keypoint_scoremap[0].detach().numpy())\n",
    "keypoint_coords = transform_cropped_coords(keypoint_coords_crop, centers, scale_crop, 224)\n",
    "\n",
    "\n",
    "img = transform0(sample_original.squeeze())\n",
    "print(img.size)\n",
    "\n",
    "# visualize\n",
    "# fig = plt.figure(1, figsize=(16, 16))\n",
    "# ax1 = fig.add_subplot(221)\n",
    "# ax2 = fig.add_subplot(222)\n",
    "# ax3 = fig.add_subplot(223)\n",
    "# ax4 = fig.add_subplot(224, projection='3d')\n",
    "# ax1.imshow(img)\n",
    "# plot_hand(keypoint_coords, ax1)\n",
    "# ax2.imshow(transform0(image_crop[0] + 0.5))\n",
    "# plot_hand(keypoint_coords_crop, ax2)\n",
    "# ax3.imshow(np.argmax(keypoint_scoremap[0].detach().numpy(), 0))\n",
    "# plot_hand_3d(keypoint_coords3d, ax4)\n",
    "# ax4.view_init(azim=-90.0, elev=-90.0)  # aligns the 3d coord with the camera view\n",
    "# ax4.set_xlim([-5, 5])\n",
    "# ax4.set_ylim([-5, 5])\n",
    "# ax4.set_zlim([-5, 5])\n",
    "# plt.show()\n",
    "transform2 = torchvision.transforms.Resize((240, 320))\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(img)\n",
    "plot_hand(keypoint_coords, plt)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left = 0, bottom = 0, right = 1, top = 1, hspace = 0, wspace = 0)\n",
    "\n",
    "pil = fig2img(fig)\n",
    "pil = transform2(pil)\n",
    "\n",
    "pil.save('test.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " img = transform0(sample_original.squeeze())\n",
    "            fig = plt.figure()\n",
    "            ax1 = fig.add_subplot()\n",
    "            plt.imshow(img)\n",
    "            plot_hand(keypoint_coords, ax1)\n",
    "            plt.axis(\"off\")\n",
    "            pil = fig2img(fig)\n",
    "            pil = transform4(pil)\n",
    "            pil.save(\"../outputs/out_video/ver0_3/{0:04d}.png\".format(i+1))\n",
    "#             plt.savefig(\"../outputs/out_video/ver0_3/{0:04d}.png\".format(i+1))\n",
    "            plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
