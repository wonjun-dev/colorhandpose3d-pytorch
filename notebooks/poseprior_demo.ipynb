{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PosePrior Demo\n",
    "\n",
    "This notebook contains code to convert `PosePrior` as implemented in \"Learning to Estimate 3D Hand Pose from Single RGB Images\" by Zimmerman et al. Their project is available at [https://github.com/lmb-freiburg/hand3d]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "\n",
    "model_path = os.path.abspath(os.path.join('..'))\n",
    "if model_path not in sys.path:\n",
    "    sys.path.append(model_path)\n",
    "    \n",
    "from colorhandpose3d.model.HandSegNet import HandSegNet\n",
    "from colorhandpose3d.model.PoseNet import PoseNet\n",
    "from colorhandpose3d.model.PosePrior import PosePrior\n",
    "from colorhandpose3d.model.ViewPoint import ViewPoint\n",
    "from colorhandpose3d.utils.general import *\n",
    "from colorhandpose3d.utils.transforms import flip_right_hand, flip_left_hand, get_rotation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models\n",
    "\n",
    "`PosePrior` depends on the output of `HandSegNet`+`PoseNet`. First define the required models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handsegnet = HandSegNet()\n",
    "posenet = PoseNet()\n",
    "poseprior = PosePrior()\n",
    "\n",
    "handsegnet.load_state_dict(torch.load('../saved/handsegnet.pth.tar'))\n",
    "posenet.load_state_dict(torch.load('..//saved/posenet.pth.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import weights from Tensorflow model\n",
    "\n",
    "The weights are saved in `pickle` format from the Tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../weights/posenet3d-rhd-stb-slr-finetuned.pickle'\n",
    "session = tf.Session()\n",
    "exclude_var_list = list()\n",
    "\n",
    "# read from pickle file\n",
    "with open(file_name, 'rb') as fi:\n",
    "    weight_dict = pickle.load(fi)\n",
    "    weight_dict = {k: v for k, v in weight_dict.items() if not any([x in k for x in exclude_var_list])}\n",
    "    \n",
    "keys = [k for k, v in weight_dict.items() if 'PosePrior' in k]\n",
    "keys.sort()\n",
    "\n",
    "[print(k, weight_dict[k].shape) for k in keys]\n",
    "    \n",
    "for name, module in poseprior.named_children():\n",
    "    key = 'PosePrior/{0}/'.format(name)\n",
    "    if key + 'biases' in weight_dict:\n",
    "        b = torch.tensor(weight_dict[key + 'biases'])\n",
    "        w = torch.tensor(weight_dict[key + 'weights'])\n",
    "        if len(w.shape) == 4:\n",
    "            w = w.permute((3, 2, 0, 1))\n",
    "        else:\n",
    "            w = w.permute(1, 0)\n",
    "        w = torch.nn.Parameter(w)\n",
    "        b = torch.nn.Parameter(b)\n",
    "        module.weight.data = w\n",
    "        module.bias.data = b\n",
    "        \n",
    "torch.save(poseprior.state_dict(), '../saved/poseprior.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and run sample\n",
    "\n",
    "Run a sample through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform0 = torchvision.transforms.ToPILImage()\n",
    "transform1 = torchvision.transforms.ToTensor()\n",
    "transform2 = torchvision.transforms.Resize((224, 224))\n",
    "# img = Image.open('../data/RHD_v1-1/RHD_published_v2/training/color/00007.png')\n",
    "img = Image.open('../outputs/in_video/0474.png')\n",
    "img = Image.open('../outputs/in_video/0597.png')\n",
    "img = Image.open('../outputs/in_video/0615.png')\n",
    "img = Image.open('../outputs/in_video/0627.png')\n",
    "img = Image.open('../outputs/in_video/0364.png')\n",
    "img = Image.open('../outputs/in_video/0380.png')\n",
    "sample_original = transform1(transform2(img)).unsqueeze(0)\n",
    "sample = sample_original - 0.5\n",
    "\n",
    "# Run through network\n",
    "output = handsegnet.forward(sample)\n",
    "\n",
    "# Calculate single highest scoring object\n",
    "test_output = single_obj_scoremap(output, 21)\n",
    "\n",
    "# Crop and resize\n",
    "centers, bbs, crops = calc_center_bb(test_output)\n",
    "crops = crops.to(torch.float32)\n",
    "crop_size = 224\n",
    "\n",
    "crops[0] *= 1.25\n",
    "scale_crop = min(max(crop_size / crops[0], 0.25), 5.0)\n",
    "image_crop = crop_image_from_xy(sample_original, centers, crop_size, scale_crop)\n",
    "mask_crop = crop_image_from_xy(test_output, centers, crop_size, scale_crop)\n",
    "\n",
    "# also take a sample crop with mean subtracted\n",
    "sample_crop = crop_image_from_xy(sample, centers, crop_size, scale_crop)\n",
    "\n",
    "# PoseNet\n",
    "keypoints_scoremap = posenet(sample_crop)\n",
    "heatmaps = F.interpolate(keypoints_scoremap, 224, mode='bilinear', align_corners=False)\n",
    "keypoints_coords = detect_keypoints(heatmaps[0].detach().numpy())\n",
    "print(keypoints_coords)\n",
    "\n",
    "img = transform0(image_crop[0])\n",
    "fig = plt.figure(1)\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.imshow(img)\n",
    "plot_hand(keypoints_coords, ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PosePrior Network\n",
    "\n",
    "PosePrior takes as input the keypoints scoremap and outputs the coordinates in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_side = torch.Tensor([[1.0, 0.0]])\n",
    "\n",
    "# PosePrior\n",
    "keypoint_coord3d = poseprior(keypoints_scoremap, hand_side)\n",
    "print(keypoint_coord3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewpoint network\n",
    "\n",
    "The final network in Zimmerman et al.'s approach estimates the rotation parameters to transform the canonical coordinates to real coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewpoint = ViewPoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../weights/posenet3d-rhd-stb-slr-finetuned.pickle'\n",
    "session = tf.Session()\n",
    "exclude_var_list = list()\n",
    "\n",
    "# read from pickle file\n",
    "with open(file_name, 'rb') as fi:\n",
    "    weight_dict = pickle.load(fi)\n",
    "    weight_dict = {k: v for k, v in weight_dict.items() if not any([x in k for x in exclude_var_list])}\n",
    "    \n",
    "keys = [k for k, v in weight_dict.items() if 'ViewpointNet' in k]\n",
    "keys.sort()\n",
    "\n",
    "# [print(k, weight_dict[k].shape) for k, v in weight_dict.items()]\n",
    "    \n",
    "for name, module in viewpoint.named_children():\n",
    "    key = 'ViewpointNet/{0}/'.format(name)\n",
    "    if key + 'biases' in weight_dict:\n",
    "        print('loading layer: {0}'.format(name))\n",
    "        print(key)\n",
    "        b = torch.Tensor(weight_dict[key + 'biases'])\n",
    "        w = torch.Tensor(weight_dict[key + 'weights'])\n",
    "        print(b.shape, w.shape)\n",
    "        \n",
    "        # tf conv2d is [kH x kW x inputC x outputC]\n",
    "        # pytorch conv2d is [outputC x inputC x kH x KW]\n",
    "        # tf fully connected is [inputC x outputC]\n",
    "        # pytorch fully connected is [outputC x inputC]\n",
    "        if len(w.shape) == 4:\n",
    "            w = w.permute((3, 2, 0, 1))\n",
    "        else:\n",
    "            w = w.t()\n",
    "        module.weight.data = w\n",
    "        module.bias.data = b\n",
    "        \n",
    "torch.save(viewpoint.state_dict(), '../saved/viewpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_params = viewpoint(keypoints_scoremap, torch.Tensor([[1.0, 0.0]]))\n",
    "print('rot_params: {0}'.format(rot_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ViewPoint output to transformation matrix\n",
    "\n",
    "The axis-angle parameters output by the ViewPoint network need to be converted to a transformation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_matrix = get_rotation_matrix(rot_params)\n",
    "print(rot_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized 3D coordinates\n",
    "\n",
    "With the rotation matrices, the normalized 3D coordinates can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_right = torch.eq(torch.argmax(hand_side, 1), 1)\n",
    "cond_right_all = torch.reshape(cond_right, [-1, 1, 1]).repeat(1, 21, 3)\n",
    "coords_xyz_can_flip = flip_right_hand(keypoint_coord3d, cond_right_all)\n",
    "coords_xyz_rel_normed = coords_xyz_can_flip @ rot_matrix\n",
    "\n",
    "\n",
    "# flip left handed inputs wrt to the x-axis for Libhand compatibility.\n",
    "coords_xyz_rel_normed = flip_left_hand(coords_xyz_rel_normed, cond_right_all)\n",
    "\n",
    "# print(coords_xyz_rel_normed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the result\n",
    "\n",
    "Now that the 3D coordinates are calculated, the result can be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hand_3d(coords_xyz, axis, color_fixed=None, linewidth='1'):\n",
    "    \"\"\"Plots a hand stick figure into a matplotlib figure. \"\"\"\n",
    "    colors = np.array([[0., 0., 0.5],\n",
    "                       [0., 0., 0.73172906],\n",
    "                       [0., 0., 0.96345811],\n",
    "                       [0., 0.12745098, 1.],\n",
    "                       [0., 0.33137255, 1.],\n",
    "                       [0., 0.55098039, 1.],\n",
    "                       [0., 0.75490196, 1.],\n",
    "                       [0.06008855, 0.9745098, 0.90765338],\n",
    "                       [0.22454143, 1., 0.74320051],\n",
    "                       [0.40164453, 1., 0.56609741],\n",
    "                       [0.56609741, 1., 0.40164453],\n",
    "                       [0.74320051, 1., 0.22454143],\n",
    "                       [0.90765338, 1., 0.06008855],\n",
    "                       [1., 0.82861293, 0.],\n",
    "                       [1., 0.63979666, 0.],\n",
    "                       [1., 0.43645606, 0.],\n",
    "                       [1., 0.2476398, 0.],\n",
    "                       [0.96345811, 0.0442992, 0.],\n",
    "                       [0.73172906, 0., 0.],\n",
    "                       [0.5, 0., 0.]])\n",
    "\n",
    "    # define connections and colors of the bones\n",
    "    bones = [((0, 4), colors[0, :]),\n",
    "             ((4, 3), colors[1, :]),\n",
    "             ((3, 2), colors[2, :]),\n",
    "             ((2, 1), colors[3, :]),\n",
    "\n",
    "             ((0, 8), colors[4, :]),\n",
    "             ((8, 7), colors[5, :]),\n",
    "             ((7, 6), colors[6, :]),\n",
    "             ((6, 5), colors[7, :]),\n",
    "\n",
    "             ((0, 12), colors[8, :]),\n",
    "             ((12, 11), colors[9, :]),\n",
    "             ((11, 10), colors[10, :]),\n",
    "             ((10, 9), colors[11, :]),\n",
    "\n",
    "             ((0, 16), colors[12, :]),\n",
    "             ((16, 15), colors[13, :]),\n",
    "             ((15, 14), colors[14, :]),\n",
    "             ((14, 13), colors[15, :]),\n",
    "\n",
    "             ((0, 20), colors[16, :]),\n",
    "             ((20, 19), colors[17, :]),\n",
    "             ((19, 18), colors[18, :]),\n",
    "             ((18, 17), colors[19, :])]\n",
    "\n",
    "    for connection, color in bones:\n",
    "        coord1 = coords_xyz[connection[0], :]\n",
    "        coord2 = coords_xyz[connection[1], :]\n",
    "        coords = np.stack([coord1, coord2])\n",
    "        if color_fixed is None:\n",
    "            axis.plot(coords[:, 0], coords[:, 1], coords[:, 2], color=color, linewidth=linewidth)\n",
    "        else:\n",
    "            axis.plot(coords[:, 0], coords[:, 1], coords[:, 2], color_fixed, linewidth=linewidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(16, 16))\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "keypoint_coords3d = coords_xyz_rel_normed.detach().numpy()\n",
    "# keypoint_coords3d = keypoint_coord3d.detach().numpy()\n",
    "keypoint_coords3d = keypoint_coords3d.squeeze()\n",
    "# plot_hand_3d(keypoint_coords3d, ax1)\n",
    "# ax1.view_init(azim=90, elev=90)\n",
    "ax1.view_init(azim=-90, elev=-90)\n",
    "# ax1.view_init(azim=-180,elev=-60)\n",
    "ax1.set_xlim([-3, 3])\n",
    "ax1.set_ylim([-4, 4])\n",
    "ax1.set_zlim([-5, 5])\n",
    "plt.show()\n",
    "print(keypoint_coords3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
